{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  },
  "colab": {
   "name": "TextClassification.ipynb",
   "provenance": [],
   "toc_visible": true
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import necessary packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import average_precision_score, confusion_matrix, accuracy_score, classification_report, roc_auc_score\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a list of characters to keep"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spec_chars = {'ا', 'ب', 'پ', 'ت', 'ث', 'ج', 'چ', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'ژ', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ',\n",
    "              'ع', 'غ', 'ف', 'ق', 'ک', 'گ', 'ل', 'م', 'ن', 'و', 'ه', 'ی', '0', '1', '2', '3', '4', '5', '6', '7', '8',\n",
    "              '9', '.', '؟'}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Removing special characters\n",
    "def rm_spec_ch(data):\n",
    "    text = ''\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            if data[i][j] not in spec_chars:\n",
    "                if data[i][j] == 'ي':\n",
    "                    text += 'ی'\n",
    "                elif data[i][j] == 'آ' or data[i][j] == 'أ' or data[i][j] == 'إ':\n",
    "                    text += 'ا'\n",
    "                elif data[i][j] == 'ك':\n",
    "                    text += 'ک'\n",
    "                else:\n",
    "                    text += ' '\n",
    "            else:\n",
    "                text += data[i]\n",
    "    data = text\n",
    "    return data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read the files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reading training data and cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "title = []\n",
    "category = []\n",
    "text = []\n",
    "\n",
    "csv.field_size_limit(300000)\n",
    "\n",
    "with open('/content/drive/My Drive/Colab Notebooks/train.csv', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter='\\t')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            line_count += 1\n",
    "        else:\n",
    "            if row:\n",
    "                title.append(rm_spec_ch(row[1]))\n",
    "                category.append(row[3])\n",
    "                text.append(rm_spec_ch(row[5]))\n",
    "                line_count += 1\n",
    "    print(f'Processed {line_count} lines.')\n",
    "\n",
    "total_char_count = 0\n",
    "total_token_count = 0\n",
    "total_frequent_token_count = 0\n",
    "\n",
    "# Decomposing into tokens (training data)\n",
    "for i in range(len(text)):\n",
    "    text[i] = text[i].replace('.', ' . ')\n",
    "    text[i] = text[i].replace('؟', ' ؟ ')\n",
    "    text[i] = re.split(' ', text[i])\n",
    "    while '' in text[i]:\n",
    "        text[i].remove('')\n",
    "    for j in range(len(text[i])):\n",
    "        if str.isnumeric(text[i][j]):\n",
    "            text[i][j] = 'N'\n",
    "\n",
    "    total_token_count += len(text[i])\n",
    "\n",
    "unigram = dict()\n",
    "\n",
    "# Populate 1-gram dictionary\n",
    "for i in range(len(text)):\n",
    "    for j in range(len(text[i])):\n",
    "        if text[i][j] in unigram:\n",
    "            unigram[text[i][j]] += 1\n",
    "        else:\n",
    "            # Start a new entry with 1 count since saw it for the first time.\n",
    "            unigram[text[i][j]] = 1\n",
    "\n",
    "        total_char_count += text[i][j].__len__()\n",
    "\n",
    "average_news_length = total_token_count // text.__len__()\n",
    "\n",
    "# Turn into a list of (word, count) sorted by count from most to least.\n",
    "unigram = sorted(unigram.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "# Frequent words are discovered only through the training data\n",
    "top_thousands = []\n",
    "freq_word_dict = dict()\n",
    "file = open('most_frequent.txt', 'w', encoding='utf-8')\n",
    "for i in range(10000):\n",
    "    file.write(unigram[i][0] + '\\n')\n",
    "    top_thousands.append(unigram[i][0])\n",
    "    total_frequent_token_count += unigram[i][1]\n",
    "    freq_word_dict[unigram[i][0]] = unigram[i][1]\n",
    "file.close()\n",
    "\n",
    "file = open('words.txt', 'w', encoding='utf-8')\n",
    "for item in unigram:\n",
    "    file.write(item[0] + '\\n')\n",
    "file.close()\n",
    "\n",
    "print('Total Number of Characters:\\t', str(total_char_count))\n",
    "print('Total Number of Words:\\t\\t', str(total_token_count))\n",
    "print('Number of Unique Words:\\t\\t', str(unigram.__len__()))\n",
    "print('Proportion of Frequent Words:\\t %', str((total_frequent_token_count / total_token_count) * 100))\n",
    "print('Average Length of News:\\t\\t', str(average_news_length), 'Words')\n",
    "\n",
    "# Replacing least frequent words with \"UNK\"\n",
    "# Using a dictionary to access elements in O(1).\n",
    "for i in range(len(text)):\n",
    "    for j in range(len(text[i])):\n",
    "        if text[i][j] not in freq_word_dict:\n",
    "            text[i][j] = 'UNK'\n",
    "\n",
    "word2index = dict()\n",
    "index2word = dict()\n",
    "char2index = dict((c, i) for i, c in enumerate(spec_chars))\n",
    "index2char = dict((i, c) for i, c in enumerate(spec_chars))\n",
    "\n",
    "pickle1 = open('word2index.pickle', 'w', encoding='utf-8')\n",
    "pickle2 = open('index2word.pickle', 'w', encoding='utf-8')\n",
    "pickle3 = open('char2index.pickle', 'w', encoding='utf-8')\n",
    "pickle4 = open('index2char.pickle', 'w', encoding='utf-8')\n",
    "\n",
    "word2index['UNK'] = 0\n",
    "index2word[0] = 'UNK'\n",
    "\n",
    "for index in range(len(unigram)):\n",
    "    word = unigram[index][0]\n",
    "    word2index[word] = index + 1\n",
    "    index2word[index + 1] = word\n",
    "\n",
    "pickle1.write(json.dumps(word2index))\n",
    "pickle2.write(json.dumps(index2word))\n",
    "pickle3.write(json.dumps(char2index))\n",
    "pickle4.write(json.dumps(index2char))\n",
    "\n",
    "pickle1.close()\n",
    "pickle2.close()\n",
    "pickle3.close()\n",
    "pickle4.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "category2code = dict()\n",
    "code2category = dict()\n",
    "\n",
    "index = 0\n",
    "for i in range(len(category)):\n",
    "    if category[i] != '' and category[i] != 'category' and category[i] not in category2code:\n",
    "        category2code[category[i]] = index\n",
    "        code2category[index] = category[i]\n",
    "        index += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filtered_title = []\n",
    "filtered_category = []\n",
    "filtered_text = []\n",
    "for i in range(len(text)):\n",
    "    if text[i].__len__() <= average_news_length:\n",
    "        for j in range(average_news_length - len(text[i])):\n",
    "            text[i].append('PAD')\n",
    "        filtered_title.append(title[i])\n",
    "        filtered_category.append(category2code.get(category[i]))\n",
    "        filtered_text.append(text[i])\n",
    "\n",
    "title = filtered_title\n",
    "category = filtered_category\n",
    "text = filtered_text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create corresponding *DataFrame*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(title, category, text)), columns=['Title', 'Category', 'Text'])\n",
    "df['Text'] = [\" \".join(news) for news in df['Text'].values]\n",
    "df['Category'][np.isnan(df['Category'])] = 10\n",
    "# df['Category'][np.isnan(df['Category'])] = np.median(df['Category'][~np.isnan(df['Category'])])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split data into sets of train and validation\n",
    "%70 --> train\n",
    "\n",
    "%30 --> validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_validation, y_train, y_validation = train_test_split(df['Text'], df['Category'], test_size=0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vectorizing the tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create *TF-IDF* matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multinomial Naïve Bayes Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_predicted = clf.predict(count_vect.transform(X_validation))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Summerize into a pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())\n",
    "                    ])\n",
    "text_clf = text_clf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_predicted = text_clf.predict(X_validation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Confusion Matrix:\\n', confusion_matrix(y_validation, y_predicted))\n",
    "print('Classification Report:\\n', classification_report(y_validation, y_predicted))\n",
    "print('Accuracy Score:', accuracy_score(y_validation, y_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Support Vector Machine Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, random_state=42))\n",
    "                     ])\n",
    "_ = text_clf_svm.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicted_svm = text_clf_svm.predict(X_validation)\n",
    "y_score = text_clf_svm.decision_function(X_validation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Confusion Matrix:\\n', confusion_matrix(y_validation, predicted_svm))\n",
    "print('Classification Report:\\n', classification_report(y_validation, predicted_svm))\n",
    "print('Accuracy Score:', accuracy_score(y_validation, predicted_svm))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grid Search\n",
    "Find the optimum model parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3)\n",
    "             }\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(X_train, y_train)\n",
    "print('Best Score:', gs_clf.best_score_)\n",
    "print('Best Parameters:', gs_clf.best_params_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Confusion matrix & Heatmap"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_validation, y_predicted)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read test data\n",
    "Reading testing data and cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "title = []\n",
    "category = []\n",
    "text = []\n",
    "\n",
    "csv.field_size_limit(300000)\n",
    "\n",
    "with open('/content/drive/My Drive/Colab Notebooks/test.csv', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter='\\t')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            line_count += 1\n",
    "        else:\n",
    "            if row:\n",
    "                title.append(rm_spec_ch(row[1]))\n",
    "                category.append(row[3])\n",
    "                text.append(rm_spec_ch(row[5]))\n",
    "                line_count += 1\n",
    "    print(f'Processed {line_count} lines.')\n",
    "\n",
    "total_char_count = 0\n",
    "total_token_count = 0\n",
    "total_frequent_token_count = 0\n",
    "\n",
    "# Decomposing into tokens (testing data)\n",
    "for i in range(len(text)):\n",
    "    text[i] = text[i].replace('.', ' . ')\n",
    "    text[i] = text[i].replace('؟', ' ؟ ')\n",
    "    text[i] = re.split(' ', text[i])\n",
    "    while '' in text[i]:\n",
    "        text[i].remove('')\n",
    "    for j in range(len(text[i])):\n",
    "        if str.isnumeric(text[i][j]):\n",
    "            text[i][j] = 'N'\n",
    "\n",
    "# Replacing least frequent words with \"UNK\"\n",
    "# Using a dictionary to access elements in O(1).\n",
    "for i in range(len(text)):\n",
    "    for j in range(len(text[i])):\n",
    "        if text[i][j] not in freq_word_dict:\n",
    "            text[i][j] = 'UNK'\n",
    "\n",
    "filtered_title = []\n",
    "filtered_category = []\n",
    "filtered_text = []\n",
    "for i in range(len(text)):\n",
    "    if text[i].__len__() <= average_news_length:\n",
    "        for j in range(average_news_length - len(text[i])):\n",
    "            text[i].append('PAD')\n",
    "        filtered_title.append(title[i])\n",
    "        filtered_category.append(category2code.get(category[i]))\n",
    "        filtered_text.append(text[i])\n",
    "\n",
    "title = filtered_title\n",
    "category = filtered_category\n",
    "text = filtered_text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create corresponding *DataFrame*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(list(zip(title, category, text)), columns=['Title', 'Category', 'Text'])\n",
    "df_test['Text'] = [\" \".join(news) for news in df_test['Text'].values]\n",
    "df_test['Category'][np.isnan(df_test['Category'])] = 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test on TEST data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Naïve Bayes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_predicted = clf.predict(count_vect.transform(df_test['Text']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Confusion Matrix:\\n', confusion_matrix(df_test['Category'], y_predicted))\n",
    "print('Classification Report:\\n', classification_report(df_test['Category'], y_predicted))\n",
    "print('Accuracy Score:', accuracy_score(df_test['Category'], y_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicted_svm = text_clf_svm.predict(df_test['Text'])\n",
    "y_score = text_clf_svm.decision_function(df_test['Text'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Confusion Matrix:\\n', confusion_matrix(df_test['Category'], predicted_svm))\n",
    "print('Classification Report:\\n', classification_report(df_test['Category'], predicted_svm))\n",
    "print('Accuracy Score:', accuracy_score(df_test['Category'], predicted_svm))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}